## Введение
В данном проекте показана реализация итогового задания после прохождения курса по `GreenPlum` с использованием `DBT` в качестве инструмента для трансформации данных. Первая часть будет вводная, с описанием некоторых интересных возможностей `DBT`. Общие подходы к разработке, реализации CI/CD и варианты взаимодействия с аналитиками данных при проектировании витрин.
Вторая часть - более детальный разбор структуры проекта, описание моделей, макросов, тестов.

## Часть №1. Общее описание проекта. Некоторые полезные возможности DBT. Подход к совместной разработке.
### Общее описание проекта
Для начала давайте посмотрим на архитектуру проекта:
![dev](/images/2025-01-07-20-10-50.png)
![ci/cd](/images/2025-01-07-20-11-33.png)
![prod](/images/2025-01-07-20-12-23.png)
В нашем случае мы имеем 2 сервера: 
- DEV - локальная машина, на которой ведется вся разработка
- PROD - удаленный сервер, на котором выполняется регламентная загрузка с помощью `CRON`
Связующим звеном является `GitHub Actions`, который берет на себя функцию `CI/CD`.
Основным инструментом является `DBT`, который позволяет реализовать концепцию `CI/CD` и имеет дополнительные преимущества, такие как Lineage, документация, тесты . . .
##### Lineage
`Lineage` - позволяет взглянуть на весь pipeline целиком. Наличие зависимостей между моделями (таблицами) помогает понять и каких источников они собираются и отследить полный путь трансформации данных для конкретной таблицы. Так же наличие зависимостей позволяет отлавливать ошибки в разработке на ранней стадии компиляции проекта. `Lineage` связан с документацией и позволяет сразу перейти к описанию таблицы. Нет необходимости делать запросы к базе данных, чтобы понять что из себя представляет таблица и какие данные там лежат.
![lineage](/images/2025-01-07-20-14-14.png)

##### Docs
При создании моделей `DBT` позволяет сразу добавить документацию к таблице: описание таблицы, описание полей, структуры, тестов, ответственного за таблицу и т.д. Документация доступна для всех в UI интерфейсе и обновляется при каждом изменении проекта.
![docs](/images/2025-01-07-20-16-48.png)
> С документацией по текущему проекту вы можете ознакомится по ссылке http://194.32.248.28:8081

##### Tests
`DBT` позволяет из коробки добавлять к моделям (таблицам) различные тесты, начиная со стандартных на проверку пустых значений, дублей или согласованности данных по ключу. Так же доступна возможность написания собственных тестов, которые отражают бизнес - логику. Такие будут более подробно разобраны во второй части. Из интересного функционала можно выделить возможность моделирования Unit тестов. Обычно в начале разработке, когда данных в базе мало и все крайние случаи не разобрать. Но можно написать Unit тесты на все ожидаемые случаи и протестировать логику работы проекта. И можем быть уверены, что на реальных данных все сработает как нужно.

### Подход к совместной разработке
#### Работа Data Engineer
Вся разработка новых моделей (таблиц) должна происходить только на `DEV` сервере. После создания модели её можно протестировать отдельно командой:
```bash
dbt run -m model_name
```
Большинство ошибок обнаруживаются на этапе компиляции модели.
Если модель по отдельности работает корректно, можно собрать весь проект целиком и проверить работу:
```bash
dbt build
```
После того как проект отрабатывает корректно и результат выполнения ожидаемый можно переходить к раскатке изменений на `PROD` сервер.

Для контроля качества кода и единообразия разработки можно использовать `pre-commit`  с проведением проверок и автоформатированием. 
[pre-commit docs](https://github.com/pre-commit/pre-commit?ysclid=m5icrxvcph61070645)  
Разумно добавить следующие проверки:
- linters для python скриптов: соблюдение стиля PEP8, удаление неиспользованных переменных и импортов, лишние комментарии, строки
- sql linter - приведения кода SQL к единому формату [sqlfluff](https://github.com/sqlfluff/sqlfluff)
- контроль наличия документации к таблицам
- автоформатирование .yml файлов документации

При команде `git commit`  будут сначала выполнятся проверки и автокоррекция. При обнаружении недостатков, которые система сама не сможет поправить, `commit` выполнен не будет. Система подскажет, что нужно исправить.
При устранении недостатков будет выполнен `commit` в локальном репозитории. 

Для доставки на `PROD` сервер нужно выполнить `push` в ветку `dev` на удаленном репозитории. После одобрения руководителя команды или архитектора изменения уже сливаются в основную ветку. 

Далее код автоматически доставляется на `PROD` сервер и начинает работу согласно регламенту. В нашем случае я использовал GitHub Actions. Подобный функционал представлен и другими платформами. 

Концептуально весь код должен разрабатываться только на `DEV` сервере. Доступ к `PROD` серверу должен быть у ограниченного числа лиц и использовать его только в экстренных случаях.
Такой подход позволяет держать `DEV` и `PROD` в актуальном синхронизированном состоянии и позволяет избежать ошибок при внедрении нового функционала. Плюс мы получаем чистый код и хорошую документацию, в которой будет несложно разобраться даже новому сотруднику

#### Подход к совместной разработке витрин
Я вижу несколько способов взаимодействия Data Analyst (DA) и Data Engineer (DE):
1. `Стандартный подход`   
DA формирует ТЗ -> DE разрабатывает, тестирует, внедряет -> DA проверяет, фиксирует замечания или изменяет немного логику -> DE исправляет . . . И так далее до релиза может пройти не одна итерация. 
Мне кажется, что это не очень оптимальное использование ресурсов DA и DE, а также увеличение времени разработки

2. `Для DE - проще, для DA - сложнее` (на первоначальном этапе)  
DA проектирует витрину, как и из каких таблиц она будет собираться. Формирует SQL запрос, который дает необходимый результат. Текст запроса сохраняет в файл `mart_name.sql`.
Создает отдельный файл `mart_name.yml`, в котором описывает (по образцу) бизнес смысл витрины, описание полей, каким образом рассчитываются метрики, владелец витрины (это больше нужно для самих аналитиков).
Далее делает `commit` (без дополнительных проверок, чтобы не нагружать аналитиков) и `push` в ветку, например, `marts`.
После чего уже DE изучает SQL код на предмет оптимальности. Смотрит план выполнения запроса, вносит корректировки при необходимости, которые не нарушают бизнес логику. Проверяет / устанавливает зависимости таблиц. После выполняет `commit` ( c pre-commit) -> `push` -> доставка на `PROD`. Витрина будет собираться согласно регламенту.

3. `Для DE - проще, для DA - проще` (требует дополнительных проверок первое время)  
Если DA слабо знает SQL и нет желания разбираться какие таблицы хранятся в АХД, то можно упростить им задачу и действовать следующим образом.  
Современные LLM модели (ChatGPT и другие) могут достаточно хорошо писать SQL запросы. Можно составить промт для такой модели, который будет содержать только структуру детального слоя (только DDL таблиц), описание таблиц и их полей. Интерфейс взаимодействия с такой моделью может быть любым.
DA может голосом, но я бы рекомендовал текстом по пунктам описал витрину, какие поля и каким образом рассчитывать. На выходе языковая модель должна выдавать 2 файла:
- `mart_name.sql` - с запросом для вычисления витрины
- `mart_name.yml` - с документацией к витрине на основе промта и запроса аналитика.  
DA проверяет запрос. Если он выполняется и все корректно с точки зрения бизнеса, то далее действует согласно сценарию, описанного в пункте 2.
На первоначальном этапе возможно понадобится улучшение промта для языковой модели. В дальнейшем можно добиться корректной работы.

### Оркестрация
В нашем примере не было смысла разворачивать отдельно `AirFlow` для одного процесса.
Регламентный запуск проводится с помощью `Cron` ежедневно.  
Что касается связки `DBT` - `AirFlow`, то их можно использовать вместе. Но к сожалению `AirFlow` не поддерживает полноценную визуализацию `Lineage DBT`. Каждый проект `DBT` (а он может быть не один в рамках одного АХД) будет отображаться в `AirFlow` в виде отдельной таски. При выполнении трансформации в `AirFlow` будут писаться подробные логи от `DBT` о ходе выполнения процесса. При ошибки, дальнейшее выполнение этой таски прекращается.
Так же есть ограничения для проектов, pipeline которых выходит за рамки одного АХД, например: связи `GreenPlum` -> `Clickhouse`. Сквозного `Lineage` не удасться построить.
Если есть такая необходимость,  то нужно разбивать на 2 проекта (2 последовательные таски `AirFlow`) либо выбрать другой окрестратор. Например, `Dugster`, который отлично сочетается с `DBT` и имеет ряд других преимуществ.

При использовании `AirFlow` нужно иметь отдельный репозиторий для DAGs и реализации подхода CI / CD. Этот вопрос уже давно решен и не должен вызвать никаких затруднений.

## Часть №2. Описание проекта
Рассмотрим файловую структуру проекта:
```bash
stores_dwh_dbt/
│
├── .github/workflows/       
│   └── deploy_to_prod.yml   # PipeLine для реализации CI/CD при помощи GitHub Actions
│
├── bash_scripts/          
│   └── daily_load.sh        # скрипт для ежедневной загрузки данных и записи логов
│
├── datasets/                # csv файлы для ежедневной генерации данных 
│
├── py_stripts/                 
│   └── daily_loader.py      # скрипт для ежедненой записи данных в базу данных источник
│
├── stores_dbt/              # проект DBT. Рассмотрим более подробно ниже
│
├── .gitignore               
├── README.md                
└── requirements.txt
```

Кратко напомним основные этапы:
- `daily_loader.py` - имитирует поступления новых данных в базу данных источника. С помощью этого скрипта записи из `csv` файлов за конкретный день записываются в советующие таблицы в базе данных
- На таблицы в источники смотрят внешние таблицы аналитического хранилища
- С этими внешними таблицами уже работает `BDT` реализуя весь pipeline трансформаций

Рассмотрим отдельно структуру проекта `DBT`. Сделаем обзор основных компонентов:
```bash
stores_dwh_dbt/stores_dbt
│
├── models/                # папки с моделями DBT
│   ├── staging/   
│   ├── dds/
│   ├── dm/
│   ├── busness_statistic/     
│   └── doc.md            # файл для расширенной документации
│
├── snapshots/            # папка для моделей с историчностью, например, SCD 2 
│
├── macros/               # папка с функциями
│
├── tests/                # папка с тестами
│            
└── ...
```

### `models` 
Модель - это основная абстракция `DBT`. Каждая модель соответствует одной таблице в базе данных. Результат модели - результат выполнения `SELECT` запроса. DDL таблиц создавать не нужно, `DBT` это сделает за нас. В моделях используется шаблонизатор `jinja`
```sql
{{ config(schema='dds', materialized='table', post_hook="{{ update_load_info_after_dds('stores') }}")}}

select
    plant::char(4),
    txt::text
from {{ ref('stores_stg') }}
```
В `config` мы можем явно указывать параметры представления: схему, тип материализации. В коннекторе для `GreenPlum` мы бы указывали тип таблицы, степень сжатия, дистрибуцию.
Так же удобно использовать `post` и `pre` hooks, для обработки дополнительных сценариев при выполнении модели. В нашем случае мы используем `post_hook` для обновления служебной таблицы `load_info`.
Кроме загрузки с полным обновлением данных (подходит для небольших справочников) существуют инкрементальные стратегии загрузки данных.
```sql
-- Инкрементальная загрузка дельты для таблицы bills_item в слой DDS
{{config(
        schema='dds',
        materialized='incremental',
        incremental_strategy='append',  
        post_hook="{{ update_load_info_after_dds('bills_item') }}")
 }}

{% if execute %}
    {% set flag =  is_not_updated('bills_item') %}
{% endif %}
-- Если `flag` = Fasle, значит данные уже обновлены и нет необходимости загружать.
-- Если `flag` = True, значит данные последней дельты не загружены. Произойдет добавление новых записей в хранилище
-- И с помощью 'post_hook' будут обновлены записи в информационной таблице `table_load_info`
select
    billnum::bigint,
    billitem::bigint,
    material::bigint,
    qty::bigint,
    netval::numeric(17, 2),
    tax::numeric(17, 2),
    rpa_sat::numeric(17, 2),
    calday::date
from {{ ref('bills_item_stg') }}
{% if is_incremental() %}
where '{{ flag }}'
{% endif %}
```
Можно написать собственную реализацию загрузки, используя `macros`. Об этом поговорим чуть ниже.
В папке с моделями необходимо разместить `.yml` файл с описанием моделей. Нужно для формирования документации.

```yml
models:
- name: bills_item
  description: "{{ doc('bills_item_stg') }}"
  tests:
  - check_daily_revenue:
      config:
        severity: warn
  columns:
  - name: billnum
    description: "Номер чека"
  - name: billitem
    description: "Позиция в чеке. Одна позиция для одного товара 'material'"
  - name: material
    description: "Уникальный индетификатор товара"
  - name: qty
    description: "Количество"
  - name: netval
    description: "Стоимость без НДС"
  - name: tax
    description: "Добавленная стоимость"
  - name: rpa_sat
    description: "Стоимость с НДС"
  - name: calday
    description: "Дата операции"
```
Этот файл содержит как минимум название модели, описание полей. В данном случае есть еще поле `description` с ссылкой на более подробное описание в формате `.md`, которое содержит пример данных из таблицы (то есть нет необходимости делать запросы к базе данных, чтобы получить представление о данных).
Тест `check_daily_revenue` проверяет бизнес-логику при загрузке данных. Об этом подробнее в разделе про тесты.
Ссылка на документацию таблицы: [dbt Docs](http://194.32.248.28:8081/#!/model/model.stores_dbt.bills_item_stg)

### `snapshots`
Это отдельная папка в проекте и отдельная схема в базе данных, которая содержит модели с историчностью. Например, в нашем случае по условию задачи таблица `promos` содержит только актуальные промо акции. Чтобы получить отчеты за прошлый период, нужно иметь актуальные на тот момент акции. Для этой таблицы реализована стратегия историчности SCD 2 с добавлением новых полей.  Ссылка на документацию таблицы: [promos](http://194.32.248.28:8081/#!/snapshot/snapshot.stores_dbt.promos_snapshot).  
Проверка изменений записей построчно не эффективна. Лучшим решением - это сравнивание хэш суммы набора полей. Для формирования хэш в `DBT` есть готовый инструмент `dbt_utils.generate_surrogate_key`, который принимает набор полей и возвращает хэш строку длиной 32 символа. Реализация историчности на таблицы выглядит очень просто:
```sql
{% snapshot promos_snapshot %}
{{ config(
        target_schema='snapshots',
        strategy='check',
        unique_key='promo_id',
        check_cols=['hash_diff'],
        invalidate_hard_deletes=true
    )
}}
select * from {{ ref('promos') }}  

{% endsnapshot %}
```

### `macros`
Macros - это альтернатива процедурного языка. С помощью макросов можно писать свои реализации загрузки, дополнительные функции и так далее. Есть возможность использовать внутренние и глобальные переменные для сокращения вычислений и лишних обращений к базе данных.
Ниже представлен вариант delta загрузки с источника.
```sql
-- Макрос для загрузки данных из источника на основе дельты(дата на источнике - дата в хранилище)
{% macro delta_load(table_name) %}

-- Сформируем запрос на получение последеней актуальной даты в целевой таблице

{% set last_load_date_query %}
select last_datetime_dds
from {{ source('postgres_db_staging', 'table_load_info') }}
where table_name = '{{ table_name }}'
{% endset %}

-- Выполним запрос и сохраним необходимую дату в переменную
{% if execute %}
{% set last_load_date = run_query(last_load_date_query).columns[0].values()[0] %}
{% endif %}

-- выполним загрузку дельты с истоника

select *
from {{ source('postgres_db_source', table_name) }}
where calday > '{{ last_load_date }}'

{% endmacro %}
```

Использование макроса выглядит следующим образом:
```sql
{{ config(schema='staging', materialized='table', post_hook="{{ update_load_info('bills_item') }}") }}

-- Загрузка дельты для таблицы bills_item
{{ delta_load('bills_item') }}
```

Использование макросов особенно удобно в системах, где нет процедурного языка, например, `ClickHouse`, `Hadoop HDFS`. . .

### `tests`
В `DBT` выделяет 3 вида тестов:
- `singular` - тесты на основе обычной логики sql запроса. Пример из документации
```sql
-- Refunds have a negative amount, so the total amount should always be >= 0.
-- Therefore return records where total_amount < 0 to make the test fail.
select
    order_id,
    sum(amount) as total_amount
from {{ ref('fct_payments') }}
group by 1
having total_amount < 0
```
Тест считается не пройденным, если запрос возвращает не `Null`.
Сингулярный значит неуниверсальный. То есть для проверки какого-то частного случая. Свой тест на одну таблицу 
- `generic` - тесты, который можно использовать для разных таблиц. Достаточно большое количество готовых тестов для проверки уникальности значений, пропущенных значений. Так же полезен тест для проверки согласованности таблиц по ключам, особенно когда мы работаем с DataVault 2.0 и вставляем данные параллельно в хабы, линки, сателлиты.
- `unit` - юнит тесты в классическом понимании. Когда нужно проверять гипотезы, которых еще нет в реальных данных.  
В зависимости от логики не пройденные могут приостанавливать дальнейшую загрузку или просто выдавать предупреждение о проваленной проверке. Можно настраивать порог, например, не более 3 пропущенных значений, свыше которого загрузка будет остановлена.
Все строки, которые не прошли проверку, записываются в отдельную схему `dbt_test__audit` и доступны для детального анализа. Что очень удобно.

Для написания собственных тестов мы используем макросы.
В данном проекте я использую 2 теста:
- `check_date` - он проверяет те случаи, когда на источнике дата по каким-то причинам больше текущей даты. 
```sql
-- тест для проверки, что дата больше текущей (невалидные/ошибочные данные)
{% test check_date(model, column_name) %}

	select *
    from {{ model }}
    where {{ column_name }} >= '{{ var("date_to") }}'
    
{% endtest %}
```
Эта ошибка критичная для достоверных отчетов, потому загрузка упадет с ошибкой. Невалидные строки запишутся в схему `dbt_test__audit`. 

- `check_plant_zero_traffic` - бизнесовый тест, который подсвечивает магазины с нулевым трафиком за день. И нужно разбираться почему магазин не работал или какой-то технический сбой. Загрузка данных и построение витрин не останавливается.
```sql
{% test check_plant_zero_traffic(model) %}
    SELECT s.plant, t.calday
    FROM {{ ref('stores') }} s
    LEFT JOIN
    (SELECT plant, calday, sum(quantity) AS daily_traffic
    FROM {{ model }}
    GROUP BY plant, calday) t
    ON s.plant = t.plant
    WHERE t.daily_traffic IS null
{% endtest %}
```
- `check_daily_revenue` - бизнесовый тест, который проверяет текущую выручку магазина. Если выручка магазина выходит за диапазон: среднее значение выручки +/- 2 стандартных отклонения (не входит в 95 % доверительный интервал), то тест выдаст информационное предупреждение. Загрузка таблиц и построение витрин не будет приостановлены. Записи не прошедшие тест, будут отдельно сохранены
```sql
{% test check_daily_revenue(model) %}

with daily_revenue_table as (
    select bh.plant, bh.calday, sum(bi.rpa_sat) as daily_revenue
    from {{ ref('bills_head_stg') }} bh
    join {{ ref('bills_item_stg') }} bi
    on bh.billnum = bi.billnum and bh.calday = bi.calday
    group by bh.plant, bh.calday
)

select drt.plant, drt.calday, drt.daily_revenue
from daily_revenue_table drt
join {{ ref('plant_revenue_avg') }} pra
on drt.plant = pra.plant
where drt.daily_revenue > pra.avg_revenue + double_std_revenue
    or drt.daily_revenue < pra.avg_revenue - double_std_revenue

{% endtest %}
```
### Пример запуска проекта `DBT`
```bash
dbt build --vars "{'date_from': '2021-01-05', 'date_to': '2021-01-06'}" --store-failures
```
Для запуска достаточно команды `dbt build`. При этом произойдет сначала компиляция проекта и только потом загрузка данных моделей, тесты . . .  
В нашем случае мы передаем дополнительные параметры:
- `--vars "{'date_from': '2021-01-05', 'date_to': '2021-01-06'}"` - эти даты влияют только на построение ежедневных отчетов. Загрузка хранилища от этого не зависит. Даже если АХД не обновлялось 5 дней, будет загружена необходимая дельта и хранилище примет актуальное состояние. Так же повторный запуск проекта никак не влияет на качество данных. Дубликаты не образуются.
- `--store-failures` - этот флаг позволяет записывать строки, которые не прошли проверки, в специальные таблицы.  
Результаты выполнения можно посмотреть в логах на удаленном сервере.

## Выводы
- `BDT` универсальный инструмент, который заслуживает внимания. Универсальный потому, что формирует общий единообразный подход к проектированию хранилищ, не зависимо на какой базе данных оно построено (Postgres, GreenPlum, ClickHouse, S3 . . .)
- Не заменим в тех системах, где нет процедурного языка
- Предоставляет возможность реализации CI / CD в разработке
- При желании аналитики могут самостоятельно разрабатывать витрины. Ведь каждая модель - это всего лишь sql запрос. Или подключить языковые модели для генерации запроса sql и документации
- Позволяет вести качественную документацию в непосредственной близости к самому проекту
- Большое количество готовых тестов и макросов. Даже есть готовые реализации для загрузки данных в хабы, линки, сателлиты в DataVault. Есть возможность написать свои или переписать исходные под свои нужды
- Наличие `Lineage` с зависимостями. Не нужно постоянно думать какая таблица от какой завит и помнить очередность загрузки. Очень удобно для отладки ошибок и понимать окуда могут поступать некорретные данные.
- Пока нет официальной поддержки GreenPlum. Но есть уже успешные кейсы реализации и адаптеры для работы в GP в свободном доступе. Или написать свой.
- AirFlow не позволяет использовать все возможности `Lineage`
