## Вместо введения
В данном проекте показана реализация итогового задания после прохождения курса по `GreenPlum` с использованием `DBT` в качестве инструмента для трансформации данных. Первая часть будет вводная, с описанием некоторых интересных возможностей `DBT`. Общие подходы к разработке, реализации CI/CD и варианты взаимодействия с аналитиками данных при проектировании витрин.
Вторая часть - более детальный разбор структуры проекта, описание моделей, макросов, тестов. Как это работает вместе и как запускать.

## Часть №1. Общее описание проекта. Некоторые полезные возможности DBT. Подход к совместной разработке.
### Общее описание проекта
Для начала давайте посмотрим на архитектуру проекта:
![dev](/images/2025-01-07-20-10-50.png)
![ci/cd](/images/2025-01-07-20-11-33.png)
![prod](/images/2025-01-07-20-12-23.png)
В нашем случае мы имеем 2 сервера: 
- DEV - локальная машина, на которой ведется вся разработка
- PROD - удаленный сервер, на котором выполняется регламентная загрузка с помощью `CRON`
Связующим звеном является `GitHub Actions`, который берет на себя функцию `CI/CD`.
Основным инструментом является `DBT`, который позволяет реализовать концепцию `CI/CD` и имеет дополнительные преимущества, такие как Lineage, документация, тесты . . .
##### Lineage
`Lineage` - позволяет взглянуть на весь pipeline целиком. Наличие зависимостей между моделями (таблицами) помогает понять и каких источников они собираются и отследить полный путь трансформации данных для конкретной таблицы. Так же наличие зависимостей позволяет отлавливать ошибки в разработке на ранней стадии компиляции проекта. `Lineage` связан с документацией и позволяет сразу перейти к описанию таблицы. Нет необходимости делать запросы к базе данных, чтобы понять что из себя представляет таблица и какие данные там лежат.
![lineage](/images/2025-01-07-20-14-14.png)

##### Docs
При создании моделей `DBT` позволяет сразу добавить документацию к таблице: описание таблицы, описание полей, структуры, тестов, ответственного за таблиц и т.д. Документация доступна для всех в красивом UI интерфейсе и обновляется при каждом изменении проекта.
![docs](/images/2025-01-07-20-16-48.png)
> С документацией по текущему проекту вы можете ознакомится по ссылке http://194.32.248.28:8081

##### Tests
`DBT` позволяет из коробки добавлять к моделям (таблицам) различные тесты, начиная со стандартных на проверку пустых значений, дублей или согласованности данных по ключу. Так же доступна возможность написания собственных тестов, которые отражают бизнес - логику. Такие будут более подробно разобраны во второй части. Из интересного функционала можно выделить возможность моделирования Unit тестов. Обычно в начале разработке, когда данных в базе мало и все крайние случаи не разобрать. Но можно написать Unit тесты на все ожидаемые случаи и протестировать логику работы проекта. И можем быть уверены, что на реальных данных все сработает как нужно.

### Подход к совместной разработке
#### Работа Data Engineer
Вся разработка новых моделей (таблиц) должна происходить только на `DEV` сервере. После создания модели её можно протестировать отдельно командой:
```bash
dbt run -m model_name
```
Большинство ошибок обнаруживаются на этапе компиляции модели.
Если модель по отдельности работает корректно, можно собрать весь проект целиком и проверить работу:
```bash
dbt build
```
После того как проект отрабатывает корректно и результат выполнения ожидаемый можно переходить к раскатки изменений на `PROD` сервер.

Для контроля качества кода и единообразия разработки необходимо использовать `pre-commit`  с проведением проверок и автоформатированием. 
[pre-commit docs](https://github.com/pre-commit/pre-commit?ysclid=m5icrxvcph61070645)
Разумно добавить следующие проверки:
- linters для python скриптов: соблюдение стиля PEP8, удаление неиспользованных переменных и импортов, лишние комментарии, строки
- sql linter - приведения кода SQL к единому формату [sqlfluff](https://github.com/sqlfluff/sqlfluff)
- контроль наличия документации к таблицам
- автоформатирование .yml файлов документации

При команде `git commit`  будут сначала выполнятся проверки и автокоррекция. При обнаружении недостатков, которые система сама не сможет поправить, `commit` выполнен не будет. Система подскажет, что нужно исправить.
При устранении недостатков будет выполнен `commit` в локальном репозитории. 

Для раскатки на `PROD` сервер нужно выполнить `push` в ветку `dev` на удаленном репозитории. После одобрения руководителя команды или архитектора изменения уже сливаются в основную ветку. 

Далее код автоматически доставляется на `PROD` сервер и начинает работу согласно регламенту. В нашем случае я использовал GitHub Actions. Подобный функционал представлен и другими платформами. 

Концептуально весь код должен разрабатываться только на `DEV` сервере. Доступ к `PROD` серверу должен быть у ограниченного числа лиц и использовать его только в экстренных случаях.
Такой подход позволяет держать `DEV` и `PROD` в актуальном синхронизированном состоянии и позволяет избежать ошибок при внедрении нового функционала. Плюс мы получаем чистый код и хорошую документацию, в которой будет несложно разобраться даже новому сотруднику

#### Подход к совместной разработке витрин
Я вижу несколько способов взаимодействия Data Analyst (DA) и Data Engineer (DE):
1.  _Стандартный подход._ 
DA формирует ТЗ -> DE разрабатывает, тестирует, внедряет -> DA проверяет, фиксирует замечания или изменяет немного логику -> DE исправляет . . . И так далее до релиза может пройти не одна итерация. 
Мне кажется, что это не очень оптимальное использование ресурсов DA и DE, а также увеличение времени разработки

2. _Для DE - проще, для DA - сложнее (на первоначальном этапе)_
DA проектирует витрину, как и из каких таблиц она будет собираться. Формирует SQL запрос, который дает необходимый результат. Текст запроса сохраняет в файл `mart_name.sql`.
Создает отдельный файл `mart_name.yml`, в котором описывает (по образцу) бизнес смысл витрины, описание полей, каким образом рассчитываются метрики, владелец витрины (это больше нужно для самих аналитиков)
Далее делает `commit` (без дополнительных проверок, чтобы не нагружать аналитиков) и `push` в ветку, например, `marts`.
После чего уже DE изучает SQL код на предмет оптимальности. Смотрит план выполнения запроса, вносит корректировки при необходимости, которые не нарушают бизнес логику. Проверяет / устанавливает зависимости таблиц. После выполняет `commit` ( c pre-commit) -> `push` -> доставка на `PROD`. Витрина будет собираться согласно регламенту.

3. _Для DE - проще, для DA - проще (требует дополнительных проверок первое время)_
Если DA слабо знает SQL и нет желания разбираться какие таблицы хранятся в АХД, то можно упростить им задачу и действовать следующим образом.

Современные LLM модели (ChatGPT и другие) могут достаточно хорошо писать SQL запросы. Можно составить промт для такой модели, который будет содержать только структуру детального слоя (только DDL таблиц) и описание таблиц и их полей. Интерфейс взаимодействия с такой моделью может быть любым.
DA может голосом, но я бы рекомендовал текстом по пунктам описал витрину, какие поля и каким образом рассчитывать. На выходе языковая модель должна выдавать 2 файла:
- `mart_name.sql` - с запросом для вычисления витрины
- `mart_name.sql` - с документацией к витрине на основе промта и запроса аналитика
DA проверяет запрос. Если он выполняется и все корректно с точки зрения бизнеса, то далее действует согласно сценарию, описанного в пункте 2.
На первоначальном этапе возможно понадобится улучшение промта для языковой модели. В дальнейшем можно добиться корректной работы.

### Оркестрация
В нашем примере не было смысла разворачивать отдельно `AirFlow` для одного процесса.
Регламентный запуск проводится с помощью `Cron` ежедневно. Более подробно об этом поговорим во второй части.
Что касается связки `DBT` - `AirFlow`, то их можно использовать вместе. Но к сожалению `AirFlow` не поддерживает полноценную визуализацию `Lineage DBT`. Каждый проект `DBT` (а он может быть не один в рамках одного АХД) будет отображаться в `AirFlow` в виде отдельной таски. При выполнении трансформации в `AirFlow` будут писаться подробные логи от `DBT` о ходе выполнения процесса. При ошибки, дальнейшее выполнение этой таски прекращается.
Так же есть ограничения для проектов, pipeline которых выходит за рамки одного АХД, например: связи `GreenPlum` -> `Clickhouse`. Сквозного `Lineage` не упасться построить.
Если есть такая необходимость,  то нужно разбивать на 2 проекта (2 последовательные таски `AirFlow`) либо выбрать другой окрестратор. Например, `Dugster`, который отлично сочетается с `DBT` и имеет ряд других преимуществ.

При использовании `AirFlow` нужно иметь отдельный репозиторий для DAGs и реализации подхода CI / CD. Этот вопрос уже давно решен и не должен вызвать никаких затруднений.
